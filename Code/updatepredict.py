# -*- coding: utf-8 -*-
"""updatepredict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1obnvPnkNyqhEpnnnrO0PEc_8531Mg24D
"""

from math import isnan
'''
written by: Catherine Breen
July 1, 2024

Training script for users to fine tune model from Breen et. al 2024
Please cite:

Breen, C. M., Currier, W. R., Vuyovich, C., Miao, Z., & Prugh, L. R. (2024).
Snow Depth Extraction From Time‚ÄêLapse Imagery Using a Keypoint Deep Learning Model.
Water Resources Research, 60(7), e2023WR036682. https://doi.org/10.1029/2023WR036682

Example run:
python src/predict.py --model_path './output1/model.pth' --img_dir './nontrained_data'  --metadata './nontrained_data/pole_metadata.csv'


'''
import sys
sys.path.append('/usr/local/lib/python3.10/dist-packages/')
import google.colab
from google.colab import drive
drive.mount('/content/drive')
import sys
sys.path.append('/content/drive/MyDrive')
import torch
import numpy as np
import cv2
import albumentations  ## may need to do pip install
import updateconfig
!pip install pretrainedmodels
import pretrainedmodels
from updatemodel import snowPoleResNet50
import argparse
import glob
import IPython
import updateutils
import pandas as pd
from tqdm import tqdm
import os
import matplotlib.pyplot as plt
from scipy.spatial import distance
import gc
import torch.quantization


def download_models():
    '''
    see the Zenodo page for the latest models
    '''
    root =  os.getcwd()
    save_path = f"{root}/models"
    if not os.path.exists(save_path):
        os.makedirs(save_path, exist_ok=True)
    url = 'https://zenodo.org/records/12764696/files/CO_and_WA_model.pth'

    # download if does not exist
    if not os.path.exists(f'{save_path}/CO_and_WA_model.pth'):
        wget_command = f'wget {url} -P {save_path}'
        os.system(wget_command)
        return print('\n models download! \n')
    else:
        return print('model already saved')

def vis_predicted_keypoints(file, image, keypoints, color=(0,255,0), diameter=15):
    file = file.split('.')[0]
    output_keypoint = keypoints.reshape(-1, 2)
    plt.imshow(image)
    for p in range(output_keypoint.shape[0]):
        if p == 0:
            plt.plot(output_keypoint[p, 0], output_keypoint[p, 1], 'r.') ## top
        else:
            plt.plot(output_keypoint[p, 0], output_keypoint[p, 1], 'r.') ## bottom
    plt.savefig(f"predictions/pred_{file}.png")
    plt.close()

def load_model(args):
    model = snowPoleResNet50(pretrained=False, requires_grad=False).to(updateconfig.DEVICE)
    # load the model checkpoint
    if args.model_path == 'models/CO_and_WA_model.pth': ## uses model from paper
        model_path = 'models/CO_and_WA_model.pth'
        checkpoint = torch.load(model_path, map_location=torch.device(updateconfig.DEVICE))
    else: ## your customized model
        checkpoint = torch.load(args.model_path, map_location=torch.device('cpu'))

    quantized_model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8)
    quantized_model.eval()
    return quantized_model

def predict(model, args, device): ##

    if not os.path.exists(f"predictions"):
        os.makedirs(f"predictions", exist_ok=True)

    Cameras, filenames = [], []
    x1s_pred, y1s_pred, x2s_pred, y2s_pred = [], [], [], []
    total_length_pixels = []
    snow_depths = []

    ## folder or directory
    snowpolefiles1 = glob.glob(f"{args.img_folder}/*")
    snowpolefiles2 = glob.glob(f"{args.img_dir}/**/*")

    ## checks for a directory
    if args.img_dir != '/content/drive/MyDrive/starting100':
        snowpolefiles = snowpolefiles2
    else: # assumes it is a camerafolder
        snowpolefiles = snowpolefiles1

    metadata = pd.read_csv(f"{args.metadata}")
    with torch.no_grad():
        for i, file in tqdm(enumerate(snowpolefiles)):
            if not os.path.exists(file) or not os.path.isfile(file):
                print(f"Warning: Skipping file '{file}' as it does not exist or is not a file.")
                continue
            image = cv2.imread(file)
            if image is None:
                continue
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            h, w, *_ = image.shape
            image = cv2.resize(image, (224,224))
            image = image / 255.0

            # again reshape to add grayscale channel format
            filename = file.split('/')[-1]
            camera_identifier = filename.split('_')[0]

            ## add an empty dimension for sample size
            image = np.transpose(image, (2, 0, 1)) ##
            image = torch.tensor(image, dtype=torch.float)
            image = image.unsqueeze(0)
            image = image.to(device)

            #######
            outputs = model(image)
            outputs = outputs.cpu().numpy()
            pred_keypoint = np.array(outputs[0], dtype='float16')

            image = image.squeeze()
            image = image.cpu()
            image = np.transpose(image, (1, 2, 0))
            image = np.array(image * 255, dtype='uint8')

            ## resize back up to original size and project predicted points onto original size
            image = cv2.resize(image, (w, h))
            pred_keypoint[0] = pred_keypoint[0] * (w / 224)
            pred_keypoint[2] = pred_keypoint[2] * (w /224)
            pred_keypoint[1] = pred_keypoint[1] * (h / 224)
            pred_keypoint[3] = pred_keypoint[3] * (h /224)

            vis_predicted_keypoints(filename, image, pred_keypoint,)
            x1_pred = np.clip(pred_keypoint[0] * (w / 224), 0, w)
            y1_pred = np.clip(pred_keypoint[1] * (h / 224), 0, h)
            x2_pred = np.clip(pred_keypoint[2] * (w / 224), 0, w)
            y2_pred = np.clip(pred_keypoint[3] * (h / 224), 0, h)

            total_length_pixel = distance.euclidean([x1_pred,y1_pred],[x2_pred,y2_pred])
            pole_length_px = total_length_pixel

            ## snow depth conversion ##
            try:
                full_length_pole_cm = metadata.iloc[i]['pole_length_cm']
                pixel_cm_conversion = metadata.iloc[i]['pixel_cm_conversion']
            except (IndexError, KeyError):
                print(f"Warning: Missing metadata for file '{file}'. Skipping snow depth calculation.")
                snow_depth = np.nan
                continue
            if not np.isnan(full_length_pole_cm) and not np.isnan(pixel_cm_conversion):
                snow_depth = (total_length_pixel * pixel_cm_conversion) / full_length_pole_cm
            else:
                print(f"Warning: Missing or invalid metadata values for file '{file}'. Skipping snow depth calculation.")
                snow_depth = np.nan

            Cameras.append(camera_identifier)
            filenames.append(filename)
            snow_depths.append(snow_depth)
            total_length_pixels.append(total_length_pixel)
            x1s_pred.append(x1_pred)
            y1s_pred.append(y1_pred)
            x2s_pred.append(x2_pred)
            y2s_pred.append(y2_pred)

            del image, outputs
            gc.collect()

    results = pd.DataFrame({'camera_id':Cameras, 'filename':filenames, \
        'x1_pred': x1s_pred, 'y1_pred': y1s_pred, 'x2_pred': x2s_pred, 'y2_pred': y2s_pred, \
                            'total_length_pixel': total_length_pixels, 'snow_depth':snow_depths})

    results.to_csv(f"predictions/results.csv")

    return results

def main():
    # Argument parser for command-line arguments:
    parser = argparse.ArgumentParser(description='Predict top and bottom coordinates.')
    parser.add_argument('--model_path', required=False, help = 'Path to model', default = 'models/CO_and_WA_model.pth')
    parser.add_argument('--img_dir', required=False, help='Path to camera image directory', default = '/content/drive/MyDrive/starting100')
    parser.add_argument('--img_folder', required=False, help='Path to camera image folder', default = "/content/drive/MyDrive/starting100")
    parser.add_argument('--metadata', required=False, help='Path to pole metadata', default = "/content/drive/MyDrive/starting100/pole_metadata.csv")
    args = parser.parse_args('')

    download_models()
    model = load_model(args)
    device = 'cpu'
    predict(model, args, device)

if __name__ == '__main__':
    main()

from google.colab import files
files.download('predictions/results.csv')
