# -*- coding: utf-8 -*-
"""updatedataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lV82tEgHPKhdr6CKNwksF9UvTArx7M0n
"""

'''
written by: Catherine Breen
June 2024

Edited by Eli Mancini
Decemver 2024

Training script for users to fine tune model from Breen et. al 2024
Please cite:

Breen, C. M., Currier, W. R., Vuyovich, C., Miao, Z., & Prugh, L. R. (2024).
Snow Depth Extraction From Timeâ€Lapse Imagery Using a Keypoint Deep Learning Model.
Water Resources Research, 60(7), e2023WR036682. https://doi.org/10.1029/2023WR036682
'''

import google
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
import typing
from typing import List, Dict, Any
import torch
import cv2
import PIL
from PIL import Image, ImageFile
import pandas as pd
import numpy as np
import io
import importlib
import sys
sys.path.append('/content/drive/MyDrive')
import updateconfig
updateconfig.ROOT_PATH = '/content/drive/MyDrive/folders/starting100'
with open('/content/drive/MyDrive/updateconfig.py', 'r') as f:
    notebook_content = f.read()
    exec(notebook_content)
importlib.reload(updateconfig)
#import config_cpu as config ## for cpu training
import updateutils
from torch.utils.data import Dataset, DataLoader
import IPython
import matplotlib.pyplot as plt
import glob
import torch
import torchvision.transforms as T
from PIL import Image
from PIL import Image, ImageFile
import albumentations as A
from torchvision.transforms import Compose, Resize, ToTensor
from sklearn.model_selection import train_test_split
import os

# Define a function to sample every third photo
## Only used for experiments
def sample_every_x(group, x):
    indices = np.arange(len(group[1]))
    every_x = len(group[1])//x
    selected_indices = indices[2::every_x]
    return group[1].iloc[selected_indices]

def train_test_split(labels, updateconfig):

    df_data = pd.read_csv('content/drive/MyDrive/starting100/labels.csv')
    print(f'all rows in df_data {len(df_data.index)}')

    training_samples = df_data.sample(frac=0.8, random_state=100) # same shuffle everytime
    valid_samples = df_data[~df_data.index.isin(training_samples.index)]

    ## check to make sure we only use images that exist
    all_images = glob.glob(updateconfig + ('/**/*.JPG'))
    filenames = [item.split('/')[-1] for item in all_images]
    valid_samples = valid_samples[valid_samples['filename'].isin(filenames)].reset_index()
    training_samples = training_samples[training_samples['filename'].isin(filenames)].reset_index()

    # save labels to output folder
    if not os.path.exists(f"{updateconfig.OUTPUT_PATH}"):
            os.makedirs(f"{updateconfig.OUTPUT_PATH}", exist_ok=True)
    training_samples.to_csv(f"{updateconfig.OUTPUT_PATH}/training_samples.csv")
    valid_samples.to_csv(f"{updateconfig.OUTPUT_PATH}/valid_samples.csv")

    print(f'# of examples we will now train on {len(training_samples)}, val on {len(valid_samples)}')

    return training_samples, valid_samples

class snowPoleDataset(Dataset):

    def __init__(self, samples, path, aug): # split='train'):
        self.data = samples
        self.path = '/content/drive/MyDrive/starting100'
        self.resize = 224

        if aug == False:
            self.transform = A.Compose([
                A.Resize(224, 224),
                ], keypoint_params=A.KeypointParams(format='xy'))
        else:
            self.transform = A.Compose([
                A.ToFloat(max_value=1.0),
                A.CropAndPad(px=75, p =1.0), ## final model is 50 pixels
                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit=0, p=0.5),
                A.OneOf([
                    A.RandomBrightnessContrast(p=0.5),
                    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),
                    A.ToGray(p=0.5)], p = 0.5),
                A.Resize(224, 224),
                ], keypoint_params=A.KeypointParams(format='xy'))

    def __len__(self):
        return len(self.data)

    def __filename__(self, index):

        filename = self.data.iloc[index]['filename']
        return filename

    def __getitem__(self, index):
        filename = self.data.iloc[index]['filename']
        image_path= os.path.join(self.path, filename)
        image = None
        e = None
        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.JPG', '.JPEG')):
          try:
            image = cv2.imread(image_path)
            if image is None or image.size == 0:
                print(f"Failed to load image: {image_path}")
                return None
            if image.ndim == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                orig_h, orig_w, _ = image.shape
            else:
                print(f"Skipping invalid image")
                return None
          except Exception as e:
            print(f"Error loading image: {image_path}")
            print(f"Error message: {str(e)}")
            return None

        # resize the image into `resize` defined above
          image = cv2.resize(image, (self.resize, self.resize))
          image = image.astype(np.float32) / 255.0
        # get the keypoints
          keypoints = self.data.iloc[index][1:][['x1','y1','x2','y2']]  #[3:7]  ### change to x1 y1 x2 y2
          keypoints = np.array(keypoints, dtype='float32')
          # reshape the keypoints
          keypoints = keypoints.reshape(-1, 2)

          keypoints = keypoints * [self.resize / orig_w, self.resize / orig_h]

          transformed = self.transform(image=image, keypoints=keypoints)
          img_transformed = transformed['image']
          keypoints = transformed['keypoints']

        # viz training data

        #updateutils.vis_keypoints(transformed['image'], transformed['keypoints'])
          image = np.transpose(img_transformed, (2, 0, 1))

          if len(keypoints) != 2:
              updateutils.vis_keypoints(transformed['image'], transformed['keypoints'])

          return {
              'image': torch.tensor(image, dtype=torch.float32),
              'keypoints': torch.tensor(keypoints, dtype=torch.float32),
              'filename': filename
          }

# get the training and validation data samples=
directory_path = f"{updateconfig.ROOT_PATH}"

labels = "/content/drive/MyDrive/starting100/labels.csv"
def my_train_test_split(labels,updateconfig): #removed .ROOT_PATH
    df_data = pd.read_csv('/content/drive/MyDrive/starting100/labels.csv')
    print(f'all rows in df_data {len(df_data.index)}')

    training_samples = df_data.sample(frac=0.8, random_state=100) # same shuffle everytime
    valid_samples = df_data[~df_data.index.isin(training_samples.index)]
    if not os.path.exists(f"{updateconfig.OUTPUT_PATH}"):
            os.makedirs(f"{updateconfig.OUTPUT_PATH}", exist_ok=True)
    training_samples.to_csv(f"{updateconfig.OUTPUT_PATH}/training_samples.csv", index=False)
    valid_samples.to_csv(f"{updateconfig.OUTPUT_PATH}/valid_samples.csv", index=False)
    return training_samples, valid_samples

training_samples, valid_samples = my_train_test_split(labels, updateconfig)

# initialize the dataset - `snowPoleDataset()`
train_data = snowPoleDataset(training_samples,
                                 f"{updateconfig.ROOT_PATH}", aug = updateconfig.AUG)  ## we want all folders

valid_data = snowPoleDataset(valid_samples,
                                 f"{updateconfig.ROOT_PATH}", aug = False) # we always want the transform to be the normal transform
camera_folder = directory_path
all_images = glob.glob(os.path.join(camera_folder, '*.JPG'))
filenames = [os.path.basename(item) for item in all_images]

print(f"Checking for images in: {updateconfig.ROOT_PATH}")
camera_folder = directory_path
\
all_images = glob.glob(os.path.join(camera_folder, '*.JPG'))
filenames = [os.path.basename(item) for item in all_images]

if not hasattr(updateconfig, 'NUM_WORKERS'):
    updateconfig.NUM_WORKERS = 2

def CustomCollate(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
  images = []
  keypoints = []
  filenames = []

  for item in batch:
    if item is not None and all(v is not None for v in item.values()):
      images.append(item['image'])
      keypoints.append(item['keypoints'])
      filenames.append(item['filename'])
    else:
      print(f"Skipping item with None values: {item}")

  if not images:
    return None
  target_shape = (2,2)
  max_width = max(image.shape[1] for image in images)
  max_height = max(image.shape[0] for image in images)
  padded_images = []
  for image in images:
      padding = [(0, max_height - image.shape[0]), (0, max_width - image.shape[1]), (0, 0)]
      padded_image = np.pad(image, padding, mode='constant', constant_values=0)
      padded_images.append(padded_image)

  # Pad keypoints to the target shape (2, 2)
  target_shape = (2, 2)
  padded_keypoints = []
  for k in keypoints:
      if k.shape != target_shape:
          if k.shape == (0, 2):
              padding = torch.full(target_shape, -1, dtype=k.dtype)
          else:
              # Calculate padding shape for each dimension
              pad_rows = max(0, target_shape[0] - k.shape[0])
              pad_cols = max(0, target_shape[1] - k.shape[1])

              # Create padding tensor
              padding = torch.full((pad_rows, pad_cols), -1, dtype=k.dtype)

              # Pad the keypoints tensor
              padding = torch.cat([
                  torch.cat([k, torch.full((k.shape[0], pad_cols), -1, dtype=k.dtype)], dim=1),
                  torch.cat([torch.full((pad_rows, k.shape[1] + pad_cols), -1, dtype=k.dtype)], dim=0),

                  ], dim=0)
              padding = padding[:target_shape[0], :target_shape[1]]

      else:
          padding = k
      padded_keypoints.append(padding)

  # Stack the padded images and keypoints into tensors
  images_tensor = torch.stack([torch.from_numpy(image) for image in padded_images], dim=0)
  keypoints_tensor = torch.stack(padded_keypoints, dim=0)
  return {
      'image': images_tensor,
      'keypoints': keypoints_tensor,
      'filename': filenames
    }

# prepare data loaders
train_loader = DataLoader(
    train_data, batch_size=updateconfig.BATCH_SIZE, shuffle=True,
    num_workers=updateconfig.NUM_WORKERS, collate_fn=CustomCollate
)
valid_loader = DataLoader(
    valid_data, batch_size=updateconfig.BATCH_SIZE, shuffle=False,
    num_workers=updateconfig.NUM_WORKERS, collate_fn=CustomCollate
)

print(f"Training sample instances: {len(train_data)}")
print(f"Validation sample instances: {len(valid_data)}")

if updateconfig.SHOW_DATASET_PLOT:
    updateutils.dataset_keypoints_plot(train_data)
    updateutils.dataset_keypoints_plot(valid_data)
